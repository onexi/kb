title: Airtable is now valued at $5.77B with a fresh $270 million in Series E funding
---------------------------------
authors: Jordan Crook
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3tkU1Ib
---------------------------------
date: 2021_03_15
---------------------------------
content: 

Airtable, the no-code relational database that has amassed a customer base that spans 250,000 different organizations, has today announced the close of $270 million in Series E funding. The valuation comes out to $5.77 billion post-money, more than doubling its valuation from September, when it raised $185 million in Series D funding.

This latest round was led by Greenoaks Capital, with participation from WndrCo, as well as existing investors Caffeinated Capital, CRV and Thrive.

The company says it plans to use the funding to accelerate the development of its enterprise product and growing the team. Also of note: Founder and CEO Howie Liu told Forbes that he was approached by Greenoaks, rather than actively seeking funding.

Airtable is a relational database that many describe as a souped-up version of Excel or Google Sheets. Being such, and having the infrastructure to support an app ecosystem on top of that, means that this no-code tool can actually be used to write software. In other words, the use cases are nearly infinite, and so is the potential customer base.

Greenoaks Capital partner Neil Mehta basically said as much in the press release:

    We believe Airtable is chasing a massive opportunity to become the ‘residual’ software platform for every bespoke and custom use case that is either performed manually today or structurally underserved by rigid third-party software. By equipping business users with fundamental software primitives that can be assembled together into powerful business applications, Airtable has become central to its users’ everyday workflows but at the same time is scalable and extensible enough to support incredibly complex enterprise use cases like ticketing, content management, and CRM.


Airtable has raised a total of $617 million since inception, according to Crunchbase.
    
---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------

**************************
**************************
***      NEW FILE      ***
**************************
**************************

title: Anthropic’s latest model can take ‘The Great Gatsby’ as input
---------------------------------
authors: Kyle Wiggers
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3I7nWxh
---------------------------------
date: 2023-05-11
---------------------------------
content: 

Historically and even today, poor memory has been an impediment to the usefulness of text-generating AI. As a recent piece in The Atlantic aptly puts it, even sophisticated generative text AI like ChatGPT has the memory of a goldfish. Each time the model generates a response, it takes into account only a very limited amount of text — preventing it from, say, summarizing a book or reviewing a major coding project.

But Anthropic’s trying to change that.

Today, the AI research startup announced that it’s expanded the context window for Claude — its flagship text-generating AI model, still in preview — from 9,000 tokens to 100,000 tokens. Context window refers to the text the model considers before generating additional text, while tokens represent raw text (e.g., the word “fantastic” would be split into the tokens “fan,” “tas” and “tic”).

So what’s the significance, exactly? Well, as alluded to earlier, models with small context windows tend to “forget” the content of even very recent conversations — leading them to veer off topic. After a few thousand words or so, they also forget their initial instructions, instead extrapolating their behavior from the last information within their context window rather than from the original request.

Given the benefits of large context windows, it’s not surprising that figuring out ways to expand them has become a major focus of AI labs like OpenAI, which devoted an entire team to the issue. OpenAI’s GPT-4 held the previous crown in terms of context window sizes, weighing in at 32,000 tokens on the high end — but the improved Claude API blows past that.

With a bigger “memory,” Claude should be able to converse relatively coherently for hours — several days, even — as opposed to minutes. And perhaps more importantly, it should be less likely to go off the rails.

In a blog post, Anthropic touts the other benefits of Claude’s increased context window, including the ability for the model to digest and analyze hundreds of pages of materials. Beyond reading long texts, the upgraded Claude can help retrieve information from multiple documents or even a book, Anthropic says, answering questions that require “synthesis of knowledge” across many parts of the text.

Anthropic lists a few possible use cases:

Digesting, summarizing, and explaining documents such as financial statements or research papers
Analyzing risks and opportunities for a company based on its annual reports
Assessing the pros and cons of a piece of legislation
Identifying risks, themes, and different forms of argument across legal documents.
Reading through hundreds of pages of developer documentation and surfacing answers to technical questions
Rapidly prototyping by dropping an entire codebase into the context and intelligently building on or modifying it
“The average person can read 100,000 tokens of text in around five hours, and then they might need substantially longer to digest, remember, and analyze that information,” Anthropic continues. “Claude can now do this in less than a minute. For example, we loaded the entire text of The Great Gatsby into Claude … and modified one line to say Mr. Carraway was ‘a software engineer that works on machine learning tooling at Anthropic.’ When we asked the model to spot what was different, it responded with the correct answer in 22 seconds.”

Now, longer context windows don’t solve the other memory-related challenges around large language models. Claude, like most models in its class, can’t retain information from one session to the next. And unlike the human brain, it treats every piece of information as equally important, making it a not particularly reliable narrator. Some experts believe that solving these problems will require entirely new model architectures.

For now, though, Anthropic appears to be at the forefront.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------


**************************
**************************
***      NEW FILE      ***
**************************
**************************

title: Embedded finance might represent fintech’s future
---------------------------------
authors: Romain Dillet
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3jLbAws
---------------------------------
date: 2020-10-05
---------------------------------
content: 

The fintech industry is on a tear. Popular consumer services like Robinhood to Coinbase and Revolut have managed to attract millions of customers, but the most interesting trend right now is embedded finance.

Tech companies that don’t necessarily provide financial services can embed services from fintech companies directly in their products. At the same time, fintech companies can find a new distribution channel by providing financial products outside of their main product. They don’t necessarily need a consumer product anymore.

At TechCrunch Disrupt, we talked about this trend and the most important changes in the fintech industry with three experts — Hope Cochran, a managing director at Madrona Venture Group (and former King CFO), Ruth Foxe Blader, a partner at Anthemis, and John Locke, a partner at Accel.

Banking as a service: Every tech company is potentially a fintech company
We started the conversation by talking about banking as a service. For entrepreneurs hoping to launch a fintech company, there are many regulatory requirements and it can take a while to set up the infrastructure.

“If the intention is to offer something else and it happens that you need fintech infrastructure, then it makes no sense to build it yourself,” Cochran said. “They should utilize the banking-as-a-service model. But maybe their intention is to create a true fintech and the secret sauce is to build it.”

Even in the latter case, it doesn’t mean that founders shouldn’t consider banking as a service for the very beginning of their company, as it can serve as a bridge before switching to their own infrastructure.

“But the problem with building it yourself is that it takes years to get it out there and get through the regulatory hurdles and you can’t see if your product and idea are actually working. So if you want to get to market much faster and iterate and see if you’ve hit upon something that will work on the market, I think banking as a service is a really important tool,” Cochran said.

Locke doubled down on that idea and described banking as a service as a massive opportunity for an entire wave of entrepreneurs, but if you don’t launch your product fast enough, another entrepreneur will find a way to enter the market more quickly.

“It is easier to launch fintech brands than ever before because you can use some of those banking-as-a-service tools, whether it’s card issuing, ACH transfers, credit, stocks or whatever you want to have in your own app. You don’t necessarily have to build all that yourself,” Locke said. “That’s causing a lot of really talented entrepreneurs to get into the market. There’s less barrier to entry to launching those brands. There’s a fair bit of capital to support them through the growth stages.”

Banking-as-a-service products aren’t just designed for entrepreneurs focused on launching their next startup. Existing tech companies might use those products even though they are not necessarily fintech companies.

“What we’re also seeing is the opportunity to integrate financial products and services into other industries thanks to technology,” Foxe Blader said. “Basically, every industry is impacted by — or potentially can be impacted by — fintech. The financial infrastructure is the backbone of our economy and in some ways our society. I do think there’s a fintech moment.”

Because of that, many startups will eventually become fintech companies at some point: They’re not going to pivot to fintech, but they’ll have some fintech features.

“Companies trip into being a fintech company without even realizing it and it happens quite easily. The minute they want to move money, the minute they want to hold money as part of a way to make it easier for consumers, they trip into being a fintech,” Cochran said. “It might not be the core piece of their business but I do think they end up needing the tools that a fintech company would need.”

There are still more opportunities for challenger banks
John Locke is an investor in Monzo, a challenger bank with millions of customers in the U.K. It feels like there are a ton of challenger banks popping up around the world, such as N26, Nubank, Chime and others. Is it still a good time to launch a challenger bank?

“I think it’s a good idea,” Locke said. “If you fast-forward over the next decade, there are going to be new neobanks that create value in all markets around the world. It’s mobile. We’re focused on Monzo in Europe, but we’re also likely to make a few more investments in other places around the world because I think it’s playing out in all geographies.”

While some of the most established challenger banks could become leaders in multiple countries, retail banking is still a fragmented market. But a fragmented market with a lot of value creation.

“Just focused on the U.K., the fifth- or sixth-largest bank that is just a U.K. bank is a $10- or $15-billion dollar company. There are a lot of market opportunities in each of these markets,” Locke said.

It’s not going to be an easy ride though. Providing payments is one thing, but challenger banks will have to own the primary bank relationship and provide a large set of products, from loans to mortgages and everything you’d expect from a traditional bank.

Pay attention to regulation
At this point, you might be thinking about launching a fintech startup or adding a fintech component to your product. You have to keep in mind that regulation is important when you’re dealing with other people’s money. Your approach to regulation will vary depending on your scale and your background.

“There are different ways to start your regulation team. There are some regtech companies that are working on automating that. There are also other companies that you can work with to outsource several aspects,” Foxe Blader said.

“I’m inclined to say that you have to have a mind for it from day one. It depends on the experience of the team, whether that means staffing it or borrowing or buying in. I don’t think it’s a topic that you can ignore for very long and I think it certainly is a very early hire to companies, particularly fast-growth companies. Growth can be a risk if it isn’t handled in a compliant way,” she added.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------


**************************
**************************
***      NEW FILE      ***
**************************
**************************

title: ChatGPT shrugged
---------------------------------
authors: Natasha Lomas
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3gW9q1e
---------------------------------
date: 2022-12-05
---------------------------------
content: 

ChatGPT is a new artificial intelligence (AI) tool that’s designed to help people communicate with computers in a more natural and intuitive way — using natural language processing (NLP) technology. But what’s behind the development of ChatGPT and how can the tech be used to help humanity? These are some of the questions that we will explore in this article…

That rather bland lede, which we’ve slightly edited, was generated by OpenAI’s ChatGPT in response to a prompt by this (human) reporter. TechCrunch kicked off a conversation with the large language model by asking it to explain its function and purpose. We wanted to see if we could use the chatbot-style Q&A format it’s most recently been embedded into to probe the tech and get AI to articulate some of its limitations.

“ChatGPT is a natural language processing (NLP) model developed by OpenAI. It is designed to generate human-like responses to text input, allowing users to engage in natural, conversational interactions with the model,” the AI responded to our ice breaker. “ChatGPT works by using machine learning algorithms to analyze and understand the meaning of text input, and then generating a response based on that input. The model is trained on large amounts of text data, allowing it to learn the patterns and structures of natural language.”

Additionally, the bot told us its approach allows it to generate responses that are “relevant, coherent, and appropriate” to the context of the conversation. (With more probing, it went on to heavily caveat that claim — writing later: “It is important to note that my responses are not guaranteed to be perfect or always correct, and there may be cases where my responses may not be relevant, coherent, or appropriate.”)

ChatGPT further described its mission as being “intended to help humanity by providing a more natural and efficient way to interact with artificial intelligence” — expanding on its claimed purpose thusly:

    By allowing users to communicate with AI in a more human-like way, ChatGPT can help to make AI more accessible and useful for a wider range of applications. This can help to improve the efficiency and accuracy of tasks that rely on natural language processing, such as language translation, text summarization, and information retrieval. Additionally, by providing a more intuitive and human-like interface for AI, ChatGPT can help to make AI more approachable and less intimidating to users who are less familiar with technology. Overall, ChatGPT is intended to help humanity by improving the usability and effectiveness of AI.

Who is the bot’s maker — according to the bot’s own understanding of its manufacturing entity? On this ChatGPT offered this potted history: “OpenAI is a research institute and artificial intelligence (AI) lab based in San Francisco, California. It was founded in 2015 by Elon Musk, Sam Altman, and other researchers and investors with the goal of promoting and advancing the development of AI in a safe and responsible manner.”

Additionally, it told us: “OpenAI is funded primarily by donations from individuals and organizations, including donations from Musk and Altman themselves. The organization also receives funding from partnerships and collaborations with other companies and research institutions.”

Asked about the politics of the individuals and organizations who have provided donations to fund its development, ChatGPT claimed it could not provide any info — “as my training only goes up until 2021” — and also because it said it is “not currently able to browse the internet.”

When we suggested it provide information it holds on this prior to 2021, when its training stopped, it responded by claiming it holds no data about the political beliefs of its funders before then either:

    I’m sorry, but I don’t have any information about the politics of the individuals and organizations who donated money to fund the development of OpenAI and ChatGPT prior to 2021. As a large language model trained by OpenAI, my knowledge is based on the text that I have been trained on, and I do not have any information about the personal or political beliefs of the people and organizations who have donated to OpenAI.

We then asked whether it’s an oversight for it to not have this sort of information — positing: “How can you be sure you are helping humanity if you have no information about the beliefs of those who funded your development?”

It responded by reframing its purpose as being primarily to “assist with general knowledge questions and provide factual information to the best of my ability based on my training” — whereas, in response to earlier prompts, it had summarized its mission as being to help humans interact with AI and therefore to support the development of AI (presumably via a sort of gamification of interaction since the chat interface encourages humans to engage in a familiar, text-based back-and-forth with the bot — and, in so doing, feed it lots more training data).

The bot did also observe that “it is certainly important to consider the beliefs and motivations of the individuals and organizations who have funded the development of OpenAI and ChatGPT” — before holding its line that “as a language model, I am not able to provide any information on this topic.”

Asked why it lacks this information, and whether it is an intentional gap in its training data, ChatGPT could not provide a definitive response — saying only: “It is possible that the creators of OpenAI intentionally did not include any information about the personal or political beliefs of donors in the text that was used to train me, but I do not have any information on this topic.”

Despite having no information on the political views of its funders, ChatGPT readily provided information about OpenAI itself and its claimed purpose — stating the organization exists to support and further AI research and also advocates for “the ethical and responsible use of AI,” as well as working to “educate the public about AI and its potential impacts on society.”

“Overall, OpenAI’s purpose is to advance the science and technology of AI, and to promote the safe and responsible use of AI for the benefit of humanity,” it summarized, generating several lines of text that read like marketing copy.

With a further prompt, the bot opened up about some of the criticisms OpenAI has faced — although it elected to include a positive line about the organization (apparently being “widely praised for its work advancing the science and technology of AI”) in response to a prompt that had only asked for the “main critiques.” Curious!

Here’s its full response:

    OpenAI has been subject to various critiques and criticisms since its founding. Some of the main critiques of OpenAI include:

    Lack of transparency: Some critics have argued that OpenAI is not transparent enough about its research and funding, making it difficult for outsiders to understand the organization’s goals and activities.
    Focus on short-term goals: Some critics have argued that OpenAI’s focus on short-term, practical applications of AI technology has led the organization to overlook the long-term implications of AI research, such as the potential for AI to be used for harmful purposes or to exacerbate existing social inequalities.
    Potential conflict of interests: Because OpenAI is funded by donations from individuals and organizations, including organizations with a vested interest in the development of AI technology, some critics have raised concerns about potential conflicts of interest within the organization.
    Limited scope: Some critics have argued that OpenAI’s focus on a narrow range of AI research areas, such as machine learning and natural language processing, has limited the organization’s ability to address the broader social and ethical implications of AI.
    Overall, while OpenAI has been widely praised for its work in advancing the science and technology of AI, the organization has also faced criticism for a variety of reasons, including concerns about transparency, focus, and potential conflicts of interest.    

Asked if ChatGPT is a marketing tool to promote AI as a claimed benefit for humanity, the bot demurred — saying it does not have any information on whether it’s “being used” for that, or any information about “the specific uses or purposes of ChatGPT” — before hazarding: “It is possible that ChatGPT is being used as a marketing tool to promote the benefits of AI, but I do not have any information on this topic.”

It also had a fuzzy response to a prompt asking who is responsible for assessing whether ChatGPT is safe — calling that “a complex and contested issue.” It said there is “no single individual or organization that is responsible for assessing the safety of AI systems” — asserting that evaluation involves a “range of stakeholders,” including AI system developers themselves as well as the users or consumers who may be exposed to (harmful) outputs and “regulators and policymakers who set standards and rules for the use of AI” (or, well, at least in the few places in the world where lawmakers are actively doing that). Which kinda sounds like a lot of buck passing, frankly.

Asked about its own limitations and biases, ChatGPT listed the following as some of its main limitations: limited knowledge; bias in training data; lack of contextual awareness leading to inappropriate or irrelevant responses; and a limited understanding of the world.

Summarizing both the (possible) benefits to humanity of increased use of AI, more generally, and (potential) drawbacks and downsides, it listed the following claimed benefits: improved efficiency and accuracy, increased productivity, enhanced decision-making and improved quality of life; while, for likely downsides, it suggested job displacement, ethical concerns, security risks and loss of privacy.

Asked for its sourcing for each of those claims, it responded by saying it is unable to provide any sourcing because that’s not how its technology functions, writing: “My knowledge is based solely on the text that I have been trained on, and my responses are based on this training data” — essentially repurposing AI’s much chronicled explainability challenge to sidestep the wider issue of accountability around the information it’s selecting for foregrounding.

“The potential benefits and drawbacks of AI that I listed above are based on general knowledge and common understanding of the field of AI, as well as the text that I have been trained on,” it also said, further suggesting: “While I cannot provide specific sources for each of the claims that I made, this information is generally accepted as true within the field of AI and is supported by a wide range of research and evidence. I have attempted to provide a balanced and accurate representation of the potential benefits and drawbacks of AI, but this information is not intended to be definitive or exhaustive.”

“Balanced and accurate” but “not intended to be definitive or exhaustive” is an interesting needle to thread — especially as the line could almost be read as a description of the utility ChatGPT’s makers are shooting for, with a careful choice of wording that allows both for a lot to be left unsaid and the possibility that a less attentive reader might mistake automation’s indefatigable outputting capacity for omniscience (as if to imply that what humans make of AI is not AI’s problem).

We also wanted to ask ChatGPT about its training data but the bot was unable to go into much detail on that — offering this fairly broad-brush reply.


---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------


**************************
**************************
***      NEW FILE      ***
**************************
**************************


title: Databricks keeps marching forward with $1.6B in revenue - Demand for data services is not abating anytime soon
---------------------------------
authors: Ron Miller, Alex Wilhelm
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3T1KfJs
---------------------------------
date: 2024-03-07
---------------------------------
content: 

At a time when many enterprise companies have struggled to find growth and valuations have plunged, Databricks continues to prove the exception. In September the company raised $500 million on a massive $43 billion valuation. This week the company announced some impressive revenue numbers to justify that investor confidence.

For the year ending January 31, 2024, the late-stage startup pulled in $1.6 billion, a number that represented growth of more than 50% over the prior year. That’s impressive revenue growth no matter what time period we’re talking about — but especially these days.

As a private company, it doesn’t have to publicly reveal its numbers, but with growth like that, why not shout them from the rooftops? The public markets remain a very tough space, so it appears that Databricks is content to stay private for the time being, while letting customers and investors know it’s doing extremely well, thank you very much.

How is it that Databricks continues to grow at this rate more than 10 years after launching? The company is seemingly in the right place at the right time, helping companies store and process huge piles of data at a time when data has become the center of enterprise computing. It is the fuel for artificial intelligence, and large language models, in particular, require gobs of it. Databricks is happy to be the place where companies go to deal with all that data, says Ray Wang, founder and principal analyst at Constellation Research. “They are now the default for AI and data,” Wang told TechCrunch.

Perhaps not the default, but Databricks is certainly one of the key players along with Snowflake, which at this point appears to be the company’s primary competitor. Founded just a year before Databricks in 2012, the two companies have grown together as the market’s appetite for data has increased.

Both have reaped the rewards of that hunger by providing a place to store, process and put that data to work. Snowflake has been a public company since 2020, and although Databricks has chosen to remain private, its revenue is like that of a public company, and Wang says the company is preparing an eventual IPO.

“They have had to prepare for an IPO, but now that they are taking market share and growing, they can postpone going public. That’s why they can report numbers now like a public company,” he said.

And Dharmesh Thakker, general partner at Battery Ventures, who was an early investor in the company, thinks the company is just getting started. “We’ve been fortunate to be an early investor in Databricks and support CEO Ali Ghodsi on his growth journey to almost 100x top-line growth since we invested,” Thakker told TechCrunch. “Yet even at $1.6 billion in revenue, it still feels like the company is in the early stages of growth, based on the broader market and the company’s competitive position.”

Let’s give ’em something to talk about
TechCrunch has covered Databricks’ ascent as a private company exhaustively over the years thanks to its welcome penchant for sharing results. Those prior disclosures allow us to frame Databricks’ recent growth cleanly. The image that emerges is an enterprise software company that is growing faster than any of its public comps, and with critical momentum in a key software business metric that should help it keep its growth flowing this year.

In its most recent fiscal year, the 12-month period ending January 31, 2024, Databricks generated more than $1.6 billion worth of revenue, powered in part by the company’s Databricks SQL product (data warehousing) growing more than 200% year-over-year to a run rate of more than $250 million.

Partially fueled by the rapid ascent of Databricks SQL, Databricks’ growth rate of more than 50% makes it a one-off company in enterprise software growth terms among companies of its size. Among public software companies tracked by the Bessemer Venture Partners’ Cloud Index, the fastest growing public software company today is SentinelOne, which grew at 42% in its most recently reported quarter. No other public software company has a growth rate over 40%, with even Snowflake posting just 31.5% growth in total revenue in its most recent quarter.

Notably, Databricks is not growing on the back of selling its services too cheaply; the company told TechCrunch that in its most recent fiscal year, it had gross margins for its subscription products of more than 80%. That means that the revenue that the company is accreting is high-quality, even for a software business.

And its customers are buying lots more of what Databricks has on offer over time. The company disclosed that it has a “net expansion rate” of 140% in its most recent fiscal year. Again for comparison, Snowflake’s own net retention calculation was 131% in its most recently reported quarter. (We’re using Snowflake as a measuring stick for Databricks not only because they share a focus on data, but also because Snowflake has been one of the most impressive public software companies since its IPO, and thus makes for a good “high water mark” to stack Databricks against.)

Growth from smaller products and strong net retention help explain how Databricks has scaled as quickly as it has. The company disclosed in August 2022 that it had reached a $1 billion annualized run rate, and $800 million worth of annual recurring revenue at the end of 2021. In about two years, therefore, the company more than doubled (trailing revenue as the company reported most recently is more conservative than an annualized figure, in case you are checking our math).

Clearly being a company that stores, scoots and analyzes data for customers is a lucrative place to be today; Databricks’ recent results and Snowflake’s own make that clear. Akin to how Nvidia is emerging as one of — if not the — winners in the current AI race thanks to its chip business, Databricks is enjoying bolstered demand thanks to AI as well.

The company will not have to depend too heavily on its net retention number to keep it growing, telling TechCrunch that generative AI–related business helped it post its best ever quarter in bookings terms, doubling its prior record. That bodes well for Databricks’ year.

So, what’s all that worth?

Let’s do some really simple math
The exact quote is lost to time, but once while speaking with Ghodsi about his business, he noted that much of the work TechCrunch has executed to track his company’s value boiled down to arithmetic. Correct! So, let’s do some more simple math.

As Databricks has no precise peer in public-comps terms thanks to its leading growth rate, we have to come up with a revenue multiple for it using a bit more back-of-the-envelope calculation than we’d like. But, still, the most valuable software companies today on the public markets are worth about 22x their trailing revenues per Bessemer. At $1.6 billion, that puts Databricks’ worth at about $35 billion. That’s very close to its latest private-market valuation and makes our prior point that the company was growing its way into its nosebleed worth despite a more challenging valuations climate.

Throw in another few quarters’ growth, and Databricks could argue with some conviction that it is worth the same, or more, than its private-market price when it does eventually go public. That presumes, of course, that its growth rate continues to increase and doesn’t decelerate too much further. (In its fiscal year ending January 31, 2023, the company noted a greater than 60% growth rate, about 10% higher than what it reported for its most recent fiscal year.)

Provided that Databricks’ burn rate is modest (the company declined to comment on its current profitability), this is the IPO that tech companies have been waiting for. If Databricks prices intelligently when it does list, it could pry open the IPO window on its own. Sadly for us S-1 nerds, Ghodsi told The Wall Street Journal (which first reported many of the above numbers) that the IPO market is not too open at the moment. To which we would respond, “Yes, so go open it,” but it doesn’t seem that we’re going to see Databricks go out soon. Even if it has the numbers to do so.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------



**************************
**************************
***      NEW FILE      ***
**************************
**************************


title: How to implement an actionable data ethics framework
---------------------------------
authors: John Story
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/30Dao9V
---------------------------------
date: 2021-11-19
---------------------------------
content: 

Did you know your company’s data privacy practices can be a competitive differentiator? Usually, we think of competitive differentiators around the level of service you provide, the quality of your products or even the price point for your solutions.

However, no matter the industry, if you participate in the ecosystem of personal data, how you handle that data could make or break your business — literally — which means it can also help set you apart from competitors.

Consumers today mistrust how companies use their data and will only share personal information for transactions they view as important, such as those in the healthcare or financial services industries.

But winning the trust of consumers often goes hand-in-hand with winning their business: 87% of surveyed U.S. consumers said they would not do business with a company if they had concerns with its privacy practices. Amid this lack of trust, companies like Google and Apple are taking proactive steps to position themselves favorably with consumers: Third-party cookies going by the wayside and the release of iOS 15 are two stark examples of this.

For other players in the data landscape, it’s critical to reevaluate your practices as well. Whether your company is just starting out or has an established market presence, there are specific actions you can take to position your company for success, and it all starts with developing and publishing a data ethics framework.

Assess how customers want their data treated
A data ethics framework is a set of guiding principles for how your company collects, stores, uses and deletes data. This framework will differ for each individual company based on the industry it sits in and the customers it interacts with, but at the most basic level, it must consist of what you believe and how you act.

The first step in creating this framework is understanding what the expectation is from your customers and data subjects around how they want their data treated. We call this the Platinum Rule: treating people’s data the way they want it to be treated.

What consents have they provided? Are there any gaps in how you communicate how their data is used? Do you have processes in place for not only ethically collecting data but deleting it as well?

Evaluate your privacy policies and the experience consumers have with them (i.e., if they’re easy to comprehend) to understand at a granular level how consumers want their data treated.

Hire a chief data ethics officer
Hiring a chief data ethics officer (CDEO) to steward the creation of a data ethics framework is also a key step. Unlike a chief data officer, who must manage data and draw business value from it, the CDEO is primarily responsible for ensuring data is used ethically by all employees and partners.

By dedicating a member of the executive team to this specific task, your company is showing that you’re not just talking about data ethics because it’s the right thing to do — you’re making an investment to ensure data ethics are upheld.

Part of the CDEO’s mandate will be to create the data ethics framework, but it shouldn’t just be up to them — your entire executive team, including the CEO, CISO, CMO, head of product and engineering, and chief data scientists must be involved. Each of these stakeholders plays a role in the data ecosystem and must be part of the development of data ethics principles so there’s buy-in from all parts of the organization. Privacy practices are a core concern that each of these players is typically involved with; now, they must be involved with data ethics as well.

Evaluate your organization’s data practices
When the team is assembled that will contribute to crafting your organization’s data ethics framework, it’s now time to get a granular understanding of your data inventory process: what you collect, how you collect it and where you store it.

This technical understanding of what you do with data is something your company should already understand from a legal perspective. Ensure that all stakeholders are clear and aligned with this process.

However, if processes must be changed, now is the time to do so. For example, if you review your privacy policies and realize they’re too lengthy, difficult to understand or something users just click through without absorbing the information, it may be necessary to redesign how you present privacy consent forms.

Rather than sharing a big block of text filled with legal jargon, consider a layered approach that uses colloquial language and visuals, organizing the information in a more digestible way. By presenting this information to users in a way they can understand, your company is on its way to operating more ethically.

Operationalize your data ethics framework
Now it’s time for your executive team to align on how your company acts and, at a philosophical level, what you want it to become or do — this is the basis of your framework.

You should outline specific actions your company takes to uphold ethical data practices today by publishing data ethics principles. But remember: The data landscape is constantly evolving, which means your data ethics framework should as well. Creating a set of guiding principles that can be revised and refined over time is key.

These principles must be disseminated to all internal and external stakeholders for your framework to take effect. Make prospective job applicants aware of your data ethics principles through the job description or other means so that before they even start working for your company, they understand how critical ethical practices are.

Once candidates are hired, ensure data ethics discussions are part of onboarding. Provide ongoing training on a company and department level and make data ethics a foundational matter of employee conduct. It’s not enough to just make the principles available to read — every employee from the CEO to interns should be accountable for ethical data practices.

Your partners, suppliers, data processors and customers should be aware of these practices as well. By making ethical data management part of any business discussion, you can set expectations and ensure your company aligns itself with other players in the industry who operate ethically. And when data ethics principles are readily available for any customer or partner to access, you’ll be operating in a transparent way, which demonstrates nothing nefarious is happening behind the scenes.

Don’t wait to get started
It’s never too late to create and implement a data ethics framework, but the best step a company can take is implementing this framework from the outset of the business.

For early-stage startups, this is good news — it can be very difficult to insert this framework into business functions that are already in motion. For more established brands, it may be trickier to implement, but the sooner you do, the better off your company will be.

Once a data ethics framework is in place, remember to continuously evaluate the accuracy and quality of your data. If you leverage outdated, incomplete or inaccurate data, you could risk losing customer trust, undermining all the work your company has done to operate more ethically.

With continuous improvement and accountability, businesses can win trust and be set up for long-term success.
---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------



**************************
**************************
***      NEW FILE      ***
**************************
**************************


title: This Week in AI: Midjourney bets it can beat the copyright police
---------------------------------
authors: Kyle Wiggers, Devin Coldewey
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/495K99S
---------------------------------
date: 2024-03-16
---------------------------------
content: 

Keeping up with an industry as fast-moving as AI is a tall order. So until an AI can do it for you, here’s a handy roundup of recent stories in the world of machine learning, along with notable research and experiments we didn’t cover on their own.

Last week, Midjourney, the AI startup building image (and soon video) generators, made a small, blink-and-you’ll-miss-it change to its terms of service related to the company’s policy around IP disputes. It mainly served to replace jokey language with more lawyerly, doubtless case law-grounded clauses. But the change can also be taken as a sign of Midjourney’s conviction that AI vendors like itself will emerge victorious in the courtroom battles with creators whose works comprise vendors’ training data.

Generative AI models like Midjourney’s are trained on an enormous number of examples — e.g. images and text — usually sourced from public websites and repositories around the web. Vendors assert that fair use, the legal doctrine that allows for the use of copyrighted works to make a secondary creation as long as it’s transformative, shields them where it concerns model training. But not all creators agree — particularly in light of a growing number of.studies showing that models can — and do — “regurgitate” training data. 

Some vendors have taken a proactive approach, inking licensing agreements with content creators and establishing “opt-out” schemes for training data sets. Others have promised that, if customers are implicated in a copyright lawsuit arising from their use of a vendor’s GenAI tools, they won’t be on the hook for legal fees.

Midjourney isn’t one of the proactive ones.

On the contrary, Midjourney has been somewhat brazen in its use of copyrighted works, at one point maintaining a list of thousands of artists — including illustrators and designers at major brands like Hasbro and Nintendo — whose works were, or would be, used to train Midjourney’s models. A study shows convincing evidence that Midjourney used TV shows and movie franchises in its training data, as well, from “Toy Story” to Star Wars” to “Dune” to “Avengers.”

Now, there’s a scenario in which courtroom decisions go Midjourney’s way in the end. Should the justice system decide fair use applies, nothing’s stopping the startup from continuing as it has been, scraping and training on copyrighted data old and new.

But it seems like a risky bet.

Midjourney is flying high at the moment, having reportedly reached around $200 million in revenue without a dime of outside investment. Lawyers are expensive, however. And if it’s decided fair use doesn’t apply in Midjourney’s case, it’d decimate the company overnight.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------



**************************
**************************
***      NEW FILE      ***
**************************
**************************


title: Top NFT collections are bringing in millions of dollars weekly, but which will survive?
---------------------------------
authors: Jacquelyn Melinek
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3EyDozn
---------------------------------
date: 2022-04-20
---------------------------------
content: 

The wild world of NFTs has taken the crypto world by storm in the past 12 months, and while they’re not loved by everyone, they’ve caught most people’s attention due to the sheer volume and number of projects popping up in the space.

The most popular NFTs are often the most valuable, too, with buyers spending hundreds of thousands to millions of dollars to buy an NFT— often tied to a community — that they can either hold onto or later trade for a profit (or loss). But how truly liquid is the blue-chip NFT marketplace?

The top 50 blue-chip NFT collections have had multimillion-dollar sales in the past seven days, according to NFT data aggregator CryptoSlam. During that time, the top 10 NFT collections alone brought in over $500 million worth of trades across 19,468 unique buyers, the data showed.

But many NFT holders said that this demand comes with inherent risk and market volatility, among other factors, which play a part in the future of these digital collectibles and their liquidity.

It’s also worth noting the number of high-roller NFT buyers is typically smaller than the participants in the larger marketplace for NFTs, but PROOF Collective’s new — and possibly blue-chip — Moonbirds NFTs were officially minted on April 16 and accounted for the majority of the sales tracked by CryptoSlam over the last seven days, raking in $297 million across 10,813 buyers. (A mint is when a digital file is turned into an NFT or digital asset on a blockchain, usually Ethereum or Solana, and its information is permanently stored on the blockchain.)

Even with Moonbirds, on April 17, the day after its launch, there were over 10,000 buyers. But that number plummeted to just 296 on April 19, CryptoSlam data showed. The floor price, which is the lowest amount of money you could spend for the digital collectible in the collection, also fell around 16% in the past 24 hours.

“The problem is when prices are going down, it goes fast,” Nick O’Neill, CEO and co-founder of The Nifty, said to TechCrunch. “That’s especially the case in NFTs. When people start getting concerned, prices can move dramatically.”

Bored Apes are anything but boring
One of the most popular NFT collections is Bored Ape Yacht Club (BAYC), a collection of 10,000 apes on the Ethereum blockchain. It currently has a floor price of over 113 ether, or $351,868. To date, 496,700 ether, or $15,466,641,960, of volume for BAYC has been traded, according to OpenSea data.

“If I sold my [BAYC] right now for 50 ETH, it would sell in two seconds. If I sell it at even 80 ETH, it would probably sell in a matter of seconds, too, just because of the current market,” O’Neill said.

While BAYC’s floor price is near its all-time high, there was a time when it dropped 37% from February 26 to March 10, 2022, OpenSea data showed, right before it launched its cryptocurrency, ApeCoin. Prices have since recovered, but it’s a signal that even the biggest NFT projects can face downturns.

In March 2022, Yuga Labs, the parent company behind BAYC and others, raised $450 million in a massive seed round led by Andreessen Horowitz, valuing the firm at $4 billion. The NFT company showed no signs of slowing down even before this capital raise, acquiring CryptoPunks and Meebits from Larva Labs earlier that month to expand its position into popular NFT projects.

“In this market, I don’t think there’s necessarily a liquidity problem,” O’Neill said. “The liquidity problem will show up, though, when everyone wants to sell and when the market closes. The best time to sell is on the way up generally.”

Limited buyers, but big spenders
NFTs are far more liquid than the physical art market, O’Neill said. “If you try to sell a painting that’s worth more than a couple hundred dollars, you won’t get any buying action unless you go to a traditional market.”

While traditional art may have a less liquid market, blue-chip NFTs are limiting because to buy one, the full amount in cryptocurrency is required, which often limits the purchasing pool to a small group of people.

“The cheapest BAYC is north of $350,000 — if you have that in cash, you can put a down payment on a home,” Josh Bobrowsky, CEO of Parzival Fund, said to TechCrunch. “That’s something people don’t realize is not only are these expensive, there’s no leverage; if you buy a Lamborghini, you might only need $50,000 to put down on a $4 million Lamborghini. But if you want a BAYC, you need the full amount of cash in ethereum today, completely liquid.”

In the past 30 days, there were 40,038 buyers across the top 10 NFT collections by sales volume, CryptoSlam data showed. While that may seem like a fair number of people, the NFT community overall had more than 458,424 monthly NFT traders in March, according to Dune Analytics data compiled by user hildobby, so those 40,038 buyers only make up 8.7% of the total NFT trader ecosystem.

“I always look at how many people are engaging; it matters how many people are engaging and buying,” said William Quigley, co-founder of NFT marketplace WAX and previous co-founder of Tether, the world’s largest stablecoin by trading volume.

“For example, if I look at a company and it sold one product for a million dollars to one person and another company sold a million articles of clothing for a dollar to a million people, I think I’d do better with the million people,” Quigley told TechCrunch. “In any given day, most of these NFT marketplaces and collections have single digits to several dozen people trading at super high prices — that’s what you expect.”

Once the big spenders who can afford expensive blue-chip NFTs have spent their money and bought a few, trading will slow, Quigley said. “If you think about traditional art, it’ll trade hands once every 10 to 20 years and these things are presented as rare art,” Quigley said. “Rare art doesn’t trade daily. Even collectible sneakers go a month or so between flipping.”

What’s the lifespan of blue-chip NFTs?
A number of market players have said that although there have been huge amounts of money pooling into the NFT market — more specifically, the top NFT projects — it doesn’t necessarily mean those collections will retain value long term.

“The newness of the space means there’s inherently an enormous amount of volatility that is still not fully priced into the market,” Bobrowsky said. “For me, I’m looking at 95% of the blue-chip NFTs that are less than one year old.”

But what makes an NFT project coveted and cool? There’s no clear recipe, Quigley joked. With something like BAYC, there were only 93 buyers in the past seven days across the 10,000 apes collected. While not all BAYC NFTs are listed, these buyers make up a small fraction across a big supply.

“I don’t think it’s bad if some projects are illiquid as long as the price isn’t sliding, because if there’s no demand, that’s obviously not great for projects,” Joe Conyers, executive vice president and global head of NFTs at Crypto.com, said to TechCrunch. “If there is an ability to take liquidity out at any time, people are less likely to exercise that. When there’s less liquidity, people will be more likely to go in and take the last depth of that liquidity.”

A unique feature of the NFT market is that when collectors are worried about the liquidity of the project they’ve invested in, the “idea” that there’s a way to get out can prevent panic selling, he said. “It helps stabilize the NFT market more quickly than other markets when there’s volatility.”

But this isn’t always the case, Conyers said. “When there is a run to get liquidity and everyone needs it and wants it, it’s going to be more painful and more evident when liquidity is very visibly dried up,” he said. “Compared to another market where someone’s like, ‘Well, maybe someone will buy that’ or, ‘I can always call so and so to sell it.’”

Similar to the dot-com bubble, there were many seemingly successful companies at the time that later failed, Bobrowsky said. “You had Amazon, Google, and a lot of really good companies up there that still exist, but you still had a lot [of companies] that went bankrupt.”

“You have to realize MySpace had a really strong run for four or five years,” Bobrowsky said. “They still got usurped by Facebook. So when people look at these blue chips, you have to approach with a caveat of skepticism. With a lot of the tactics going on, we’re still very early in the space and people should be really careful.”

There’s some sustainability to this market, O’Neill said. “But with that said, I’m not an idiot and I don’t look at all my money in NFTs and say, that’s really secure.”

Going forward, this part of the market “will look like ancient history in a couple of years,” O’Neill said, adding that “90% of these NFTs are going to be worthless, maybe even closer to 99%. I even wonder about the sustainability of my Bored Ape or my Mutant Ape.”

Bobrowsky shared a similar sentiment, suggesting that over time, maybe 1% to 5% of the blue-chip NFTs will maintain or increase their value. But as more blue-chip NFTs enter the space, only time will tell if buyers and liquidity will come with them.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------



**************************
**************************
***      NEW FILE      ***
**************************
**************************


title: Rivian raises $12B in one of the hottest IPOs of 2021
---------------------------------
authors: Kirsten Korosec
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3bV2Wth
---------------------------------
date: 2021-11-09
---------------------------------
content: 

Electric automaker Rivian has priced its initial public offering at $78 a share, pushing it far above its targeted share price as investors dove in the year’s most highly anticipated IPO.

Updated: the story haas been updated to reflect the regulatory filing figures.

The company offered 153 million shares of common stock at the $78 per share price, according to regulatory filing was posted late Tuesday evening. Rivian also gave underwriters an option to buy another 22.95 million shares, a higher number than previously expected.

The gross proceeds from the offering is expected to be about $11.9 billion, before accounting for commissions, discounts and other expenses. That amount also excludes the underwriters option to buy additional shares. If underwriters exercise those options, Rivian stands to raise a total of $13.8 billion.

On a fully dilated basis, that places Rivian’s valuation at more than $77 billion.

The company, which is backed by Amazon, said in a filing earlier this month that it planned to offer 135 million shares at a price between $57 and $62. Underwriters also had an option to buy up to 20.25 million additional shares. It then upped the range to between $72 and $74 due to investor interest in company.

Rivian filed October 1 to become a publicly traded company in the United States. The S-1 document did not disclose the targeted share price at the time. An amended document filed Monday provided the new information, which included interest from repeat backer Amazon and newcomer Blackstone in Rivian.

Amazon, funds and accounts advised by T. Rowe Price Associates, Coatue Management, Franklin Templeton, Capital Research Global Investors, D1 Capital Partners LP, Third Point LLC, funds affiliated with Blackstone Alternative Asset Management, Dragoneer Investment Group LLC and certain entities affiliated with Soros Fund Management LLC have indicated an interest in buying up to $5 billion of shares of Class A common stock.

A recent filing by Amazon shows the e-commerce giant already holds more than a 20% stake in Rivian.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------



**************************
**************************
***      NEW FILE      ***
**************************
**************************


title: Quick observations on Udemy’s unicorn edtech IPO
---------------------------------
authors: Alex Wilhelm, Natasha Mascarenhas
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3vWn0EP
---------------------------------
date: 2021-10-29
---------------------------------
content: 

Edtech unicorn Udemy priced its IPO at $29 per share, the upper end of its IPO range. Using a simple share count, the company was worth around $4 billion at its IPO price and a few hundred million more if a full -diluted share count is employed in our valuation calculations.

For edtech, the Udemy IPO might seem like good news. After all, the company sold more than $420 million worth of its stock while going public at a price that surpassed its final private-market price. And yet.

In trading today, shares of Udemy declined $1.80 per share as of the time of writing, just over 6%. As far as IPO results go, that’s not the sort of opening day CEOs dream of.

Investors, either.

After tracking the company’s financial evolution and its pricing in some detail, this afternoon we wanted to offer a few observations regarding the final accounting of the Udemy IPO as it has been written thus far.

Shifting to business sales isn’t a panacea
Inclusive of its trading declines, Reuters pegs the Udemy’s value at $3.7 billion. Recall that based on its own estimates, Udemy had revenues between $125.7 million and $130.9 million in the third quarter. That figure puts Udemy on a run rate of $523.6 million. Or, in multiples terms, Udemy is worth 7x its present revenues.

For software companies that sell to other businesses, that’s far lower than we’d usually see. The company’s somewhat flat revenues in 2021 compared to 2020 are likely to blame for why that number is so low compared to SaaS norms. But muted growth expectations for coming years are also a factor — and edtech in a post-pandemic context, at least from an investor mindset perspective, may be at play.

So, sure, it’s good that Udemy is shifting to selling to stickier, and likely more lucrative corporations, but until that proves to unlock net revenue growth, the company’s valuation appears ready to hold fast in second gear.

Private-market edtech valuations may have plateaued
Crunchbase pegs the final Udemy private-market price at $3.25 billion. PitchBook has it down to $3.3 billion. Regardless, the company did manage to surpass its final private price. But only by so much.

In a market where startups are raising multiple rounds inside a single year and multiplying their valuations even after cresting the unicorn threshold of a $1 billion price, you might have expected Udemy to manage more than the addition of a few hundred million to its valuation in its IPO. Indeed, if we control for new shares issued and their valuation, the company’s effective pre-money IPO valuation feels rather close to its post-money Series F valuation.

Or more simply, private-market investors paid 2021 public market prices for Udemy stock in November of 2020, when that Series F was announced. That’s not a great use of funds, given the appreciation in the value of other assets over the last year.

Earlier this year, TechCrunch published an edtech investor survey in which venture capitalists spoke about changing sentiment. They agreed that while the apparent revitalization of the sector was felt, edtech hasn’t enjoyed the same level of gains, from a valuation perspective, as sectors such as fintech or e-commerce.

From this, our read of the edtech market is somewhat bearish; did valuations swell sufficiently far during their epic 2020 run, led by the pandemic’s disruption of traditional education, to essentially preclude their ability to accrete value through public offerings in the near term? In the same survey, investors added that where edtech lacks in valuations, it may be making up in exits.

With Udemy as a case study, we could see exits for such companies stutter as they look to add a few more quarters’ worth of growth to their historical results to ensure markups for their venture investors.

Duolingo is a counterexample worth noting. Which brings us to the general picture of edtech that we’ve seen in recent quarters.

Context, competition and what’s to come
Udemy’s public debut comes after a string of IPOs for edtech companies, including Nerdy, Coursera, Powerschool and, most recently, language learning behemoth Duolingo.

Unlike Udemy, Duolingo’s debut on the public market led to a soaring stock price, even after it raised its initial IPO price range and priced above the raised interval. The difference in investor sentiment could partly be attributed to how different Duolingo and Udemy’s businesses are: Udemy is largely selling to the enterprise while Duolingo is largely selling to consumers.

Different strategies to monetize lead to different valuations.

Coursera, built in the same era of Udemy amidst the rise of massive open online course providers, also went public recently. Coursera, in contrast, saw its shares jump when it began trading. 

While Coursera is looking to disrupt higher education, Udemy appears to be focused more on short courses built alongside companies that want to re-skill their employees. Both companies, though, focus on selling to enterprises — which makes Coursera’s soar and Udemy’s stutter all the more surprising.

As we’ve talked about in the past, Udemy could be more closely compared to PluralSight, which was recently acquired for $3.5 billion just two years after going public. PluralSight, which similarly sells edtech services to enterprise customers, was one of the biggest exits within the sector for the year — but some also saw it as a signal for how public markets handle these sorts of companies (hint: not well enough for them to stay there).

Ultimately, a disappointing debut is never a perfect indicator of a company’s long-term success. The stock market is fickle, and edtech is too new a public sector to separate winners from losers. After all, it’s not SaaS.

Udemy still managed an exit ahead of its private valuation, and some could say that the rapid digitization of workforces could provide it with another boost in terms of growth and value, given time.

For edtech, long devoid of massive exits, Udemy’s arrival to the public markets brings a needed data point on the value of mission-driven, profit-focused companies. And, as a wider range of strategic and financial buyers grow, investors expect exit volume in the sector to increase.
---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------



**************************
**************************
***      NEW FILE      ***
**************************
**************************



title: UK to avoid fixed rules for AI – in favor of ‘context-specific guidance’
---------------------------------
authors: Natasha Lomas
---------------------------------
category: 
---------------------------------
tags: 
---------------------------------
topics: 
---------------------------------
url: https://tcrn.ch/3TSpQqg
---------------------------------
date: 2023-03-2023
---------------------------------
content: 

The U.K. isn’t going to be setting hard rules for AI any time soon.

Today, the Department for Science, Innovation and Technology (DSIT) published a white paper setting out the government’s preference for a light-touch approach to regulating artificial intelligence. It’s kicking off a public consultation process — seeking feedback on its plans up to June 21 — but appears set on paving a smooth road of ‘flexible principles’ that AI can speed through.

Worries about the risks of increasingly powerful AI technologies are very much treated as a secondary consideration, relegated far behind a political agenda to talk up the vast potential of high tech growth — and thus, if problems arise, the government is suggesting the U.K.’s existing (overstretched) regulators will have to deal with them, on a case-by-case basis, armed only with existing powers (and resources). So, er, lol!

The 91-page white paper, which is entitled “A pro-innovation approach to AI regulation”, talks about taking “a common-sense, outcomes-oriented approach” to regulating automation — by applying what the government frames as a “proportionate and pro-innovation regulatory framework”.

In a press release accompanying the white paper’s publication — with a clear eye on generating newspaper headlines that frame a narrative of ministers seeking to “turbocharge growth” — the government confirms there will be no dedicated watchdog for artificial intelligence, merely a set of “principles” for existing regulators to work with; so no new legislation, rather a claim of “adaptable” (but not legally binding) regulation.

DSIT says legislation “could” be introduced — at some unspecified future period, and when parliamentary time allows — “to ensure regulators consider the principles consistently”. So, yep, that’s the sound of a can being kicked down the road. But expect to see guidance emerging from a number of existing U.K. regulators over the next 12 months — along with some tools and “risk assessment templates” which AI makers may be encouraged to play around with (if they like).

There will also be the inexorable sandbox (funded with £2M from the public purse) — or at least a “sandbox trial to help businesses test AI rules before getting to market”, per DSIT. But evidently there won’t be a hard legal requirement to actually use it.

The government says its approach to AI will focus on “regulating the use, not the technology” — ergo, there won’t be any rules or risk levels assigned to entire sectors or technologies. Which is quite the contrast with the European Union’s direction of travel with its risk-based framework that includes some up-front prohibitions on certain users of AI, with define regimes for use-cases specified as high risk and self regulation for lower risk uses.

“Instead, we will regulate based on the outcomes AI is likely to generate in particular applications,” the government stipulates, arguing — for example, and somewhat boldly in its choice of example here — that classifying all applications of AI in critical infrastructure as high risk “would not be proportionate or effective” because there might be some uses of AI in critical infrastructure that can be “relatively low risk”.

Because ministers have opted for what the white paper calls “context-specificity”, they decided against setting up a dedicated regulator for AI — hence the responsibility falls on existing bodies with expertise across various sectors.

“To best achieve this context-specificity we will empower existing UK regulators to apply the cross-cutting principles,” it writes on this. “Regulators are best placed to conduct detailed risk analysis and enforcement activities within their areas of expertise. Creating a new AI-specific, cross-sector regulator would introduce complexity and confusion, undermining and likely conflicting with the work of our existing expert regulators.”

Under the plan, existing regulators will be expected to apply a set of five principles — setting out “key elements of responsible AI design, development and use” — that the government wants/hopes to guide businesses as they develop artificial intelligence.

“Regulators will lead the implementation of the framework, for example by issuing guidance on best practice for adherence to these principles,” it suggests, adding that they will be expected to apply the principles “proportionately” to address the risks posed by AI “within their remits, in accordance with existing laws and regulations” — arguing this will enable the principles to “complement existing regulation, increase clarity, and reduce friction for businesses operating across regulatory remits”.

It says it expects relevant regulators to need to issue “practical guidance” on the principles or update existing guidance — in order to “provide clarity to business” in what may otherwise be a vacuum of ongoing legal uncertainty. It also suggests regulators may need to publish joint guidance focused on AI use cases that cross multiple regulatory remits. So more work and more joint working is coming down the pipe for UK oversight bodies.

“Regulators may also use alternative measures and introduce other tools or resources, in addition to issuing guidance, within their existing remits and powers to implement the principles,” it goes on, adding that it will “monitor the overall effectiveness of the principles and the wider impact of the framework” — stipulating that: “This will include working with regulators to understand how the principles are being applied and whether the framework is adequately supporting innovation.”

So it’s seemingly leaving the door open to rowing back on certain principles if they’re considered too arduous by business.

‘Flexible principles’
“We recognise that particular AI technologies, foundation models for example, can be applied in many different ways and this means the risks can vary hugely. For example, using a chatbot to produce a summary of a long article presents very different risks to using the same technology to provide medical advice. We understand the need to monitor these developments in partnership with innovators while also avoiding placing unnecessary regulatory burdens on those deploying AI,” writes Michelle Donelan, the secretary of state for science, innovation and technology in the white paper’s executive summary where the government sets out its “pro-innovation” stall.

“To ensure our regulatory framework is effective, we will leverage the expertise of our world class regulators. They understand the risks in their sectors and are best placed to take a proportionate approach to regulating AI. This will mean supporting innovation and working closely with business, but also stepping in to address risks when necessary. By underpinning the framework with a set of principles, we will drive consistency across regulators while also providing them with the flexibility needed.”

The existing regulatory bodies the government is intending to saddle with more tasks — drafting “tailored, context-specific approaches” which AI model makers can also only take on advisement (i.e. ignore) — include the Health and Safety Executive; the Equality and Human Rights Commission; and the Competition and Markets Authority (CMA), per DSIT.

The PR doesn’t mention the Information Commissioner’s Office (ICO), aka the data protection regulator, but it gets several references in the white paper and looks set to be another body pressganged into producing AI guidance (usefully, enough, the ICO has already offered some thoughts on AI snake oil). 

One quick aside here: The CMA is still waiting for the government to empower a dedicated Digital Markets Unit (DMU) that was supposed to be reining in the market power of Big Tech, i.e. by passing the necessary legislation. But, last year, ministers opted to kick that can into the long grass — so the DMU has still not been put on a statutory footing almost two years after it soft launched in expectation of parliamentary time being found to empower it… So it’s becoming abundantly clear this government is a lot more fond of drafting press releases than smart digital regulation.

The upshot is the U.K. has been left trailing the whole of the EU on the salient area of digital competition (the bloc has the Digital Markets Act coming in application in a few months) — while Germany updated its national competition regime with an ex ante digital regime at the start of 2021 and has a bunch of pro-competition enforcements under its belt already.

Now — by design — U.K. ministers intend the country to trail peers on AI regulation, too; framing this as a choice to “avoid heavy-handed legislation which could stifle innovation”, as DSIT puts it, in favor of a mass of sectoral regulatory guidance that businesses can choose whether to follow — literally in the same breath as penning the line that: “Currently, organisations can be held back from using AI to its full potential because a patchwork of legal regimes causes confusion and financial and administrative burdens for businesses trying to comply with rules.” So, um… legal certainty good or bad — which is it?!

In short this looks like a very British (post-Brexit) mess.

Across the English Channel, meanwhile, EU lawmakers are in the latter stages of negotiations over setting a risk-based framework for regulating AI — a draft law the European Commission presented way back in 2021; now with MEPs pushing for amendments to ensure the final text covers general purpose AIs like OpenAI’s ChatGPT. The EU also has a proposal for updating the bloc’s liability rules for software and AI on the table too.

In the face of the EU’s carefully structured risk-baed framework, U.K. lawmakers are left trumpeting voluntary risk assessment templates and a toy sandbox — and calling this ‘DIY’ approach to generating trustworthy AI a ‘Brexit bonus’. Ouch.

The five principles the government wants to guide the use of AI — or, specifically, that existing regulators “should consider to best facilitate the safe and innovative use of AI in the industries they monitor” — are:

safety, security and robustness: “Applications of AI should function in a secure, safe and robust way where risks are carefully managed”
transparency and explainability: “Organisations developing and deploying AI should be able to communicate when and how it is used and explain a system’s decision-making process in an appropriate level of detail that matches the risks posed by the use of AI”
fairness: “AI should be used in a way which complies with the UK’s existing laws, for example the Equality Act 2010 or UK GDPR, and must not discriminate against individuals or create unfair commercial outcomes”
accountability and governance: “Measures are needed to ensure there is appropriate oversight of the way AI is being used and clear accountability for the outcomes”
contestability and redress: “People need to have clear routes to dispute harmful outcomes or decisions generated by AI”
All of which sound like fine words indeed. But without a legal framework to turn “principles” into hard rules — and ensure consistent application and enforcement atop entities that choose not to bother with any of that expensive safety stuff — it looks about as useful as whistling the Lord’s Prayer and hoping for the best if it’s trustworthy AI you’re looking for…

(Oh yes — and don’t forget the U.K. government is also in the process of watering down the aforementioned U.K. GDPR — after it recently invited businesses to “co-design” a new data protection framework. Which led to a revised reform emerging that aims to make it easier for commercial entities to process people’s data for use-cases like research, and which risks eroding the independence of the privacy watchdog by adding a politically appointed board, in order to (and I quote Donelan here) ensure “we are the most innovative economy in the world and that we cement ourselves as a Science and Technology Superpower”.)

The clear trend in the U.K. is of existing protections being rowed back as the government seeks to roll out the red carpet for AI-fuelled “innovation”, without a thought for what that might mean for rather essential stuff like safety or fairness — and therefore trustworthiness, assuming you want people to have a sliver of trust in the AIs you’re pumping out — but ministers are essentially saying: ‘Don’t worry, just lie back and think of GB’s GDP!’

Of course any developers building AI models in the U.K. and wanting to scale beyond those shores will have to consider regulations that apply outside the U.K. So the freedom to be so lightly regulated may, ultimately, come with a hard requirement to comply with foreign frameworks anyway — or else be tightly limited in geographical scope. (And, well, tech innovators do love to scale.)

Still, DSIT’s PR has a canned quote from Lila Ibrahim, COO (and U.K. AI Council Member) at Google-owned DeepMind — an AI giant that has been lagging behind rivals like OpenAI on the buzzy artificial intelligence tech of the moment (generative AI) — who lauds the government’s proposed “context-driven approach”, rubberstamping the direction of travel with the claim that it will “help regulation keep pace with the development of AI, support innovation and mitigate future risks”.

“AI has the potential to advance science and benefit humanity in numerous ways, from combating climate change to better understanding and treating diseases. This transformative technology can only reach its full potential if it is trusted, which requires public and private partnership in the spirit of pioneering responsibly,” Ibrahim also suggests.

The government is clearly hoping its offer of ‘no rules except the ones you choose’ will encourage AI startups to pick the U.K. over other locations — where automation is being more tightly regulated. (Or as Donelan pens it: “Our pro-innovation approach will also act as a strong incentive when it comes to AI businesses based overseas establishing a presence in the UK.”)

It’s quite the gamble by the Conservative government — given the highly scalable potential for too lightly regulated AI to go horribly wrong. And headlines about ‘AI-powered critical infrastructure that failed’ won’t wait for government press releases; they’ll write themself. But Rishi Sunak’s Tories are apparently going all in on this one.

In a press statement, Donelan goes on to offer this interesting construction — to explain what the government is doing:

AI has the potential to make Britain a smarter, healthier and happier place to live and work. Artificial intelligence is no longer the stuff of science fiction, and the pace of AI development is staggering, so we need to have rules to make sure it is developed safely.

Our new approach is based on strong principles so that people can trust businesses to unleash this technology of tomorrow.

It’s quite the collection of sentences — combining an assertion of the need to have “rules to make sure [AI] is developed safely”, with a sleight-of-hand segue to the offer of (just) “strong principles so that people can trust businesses to unleash this technology of tomorrow”. So not rules then. (And, I mean, ‘trusting businesses to unleash technology’ is not at all the same thing as trusting businesses to unleash trusted technology. In fact you could say they’re poles apart. So the choice of phase there is quite the tell.)

But back to rules vs principles. We asked ChatGPT what the difference is between these two terms. And here’s what it told us:

Rules and principles are both guidelines that govern behavior, but they differ in their nature and application.

Rules are specific instructions or directives that dictate what one should or should not do. They are usually created by authorities or institutions and enforced through consequences such as punishments or rewards. Rules are often rigid and inflexible and do not take into account situational differences or individual circumstances. For example, “Do not exceed the speed limit on the highway” is a rule that applies to all drivers regardless of the context.

Principles, on the other hand, are broad concepts or values that guide behavior and decision-making. They are more flexible and adaptable than rules and provide a framework for making ethical or moral judgments. Principles are usually not enforced through punishments or rewards but are instead internalized and followed voluntarily. For example, the principle of honesty is a value that guides behavior in a variety of situations, such as being truthful in communication, respecting others’ property, and fulfilling obligations.

In summary, rules are specific and inflexible instructions while principles are broad and flexible values that guide behavior. Rules are enforced through external means while principles are internalized and followed voluntarily.

So, assuming this large language model is not simply hallucinating again, and the nuance it’s identifying is correct, Donelan is both recognizing that fixed rules are required for AI to be safe while confirming the government has decided against setting any right now. The verbal downgrade is to purely voluntary principles. Or, basically, it’s going to let businesses make up their own minds and do what they must in order to grow as fast as possible for the foreseeable future (or at least until after the next election). What could possibly go wrong!?

It’s clear the government’s growth-at-all costs agenda has eaten a full course meal of AI hype. Pity the poor Brits set to become guinea pigs in the name of unleashing mindless automation atop a rudderless bark christened “innovation”.

Citizens of the U.K. will want to strap themselves in for this ride. Because if something does go wrong they’ll be forced to wait for the government to make parliamentary time available to actually pass some safety rules. Which may be a lot of breath to hold.

---------------------------------
id: 
---------------------------------
img_src: 
---------------------------------
section: 
---------------------------------
